\documentclass[11pt]{article}
\usepackage{url}
\setlength\topmargin{-0.6cm}   
\setlength\textheight{23.4cm}
\setlength\textwidth{17.0cm}
\setlength\oddsidemargin{0cm} 
\begin{document}
\title{Ling 572 HW1 }
\author{Daniel Campos  \tt {dacampos@uw.edu}}
\date{01/16/2019}
\maketitle 
\section{ Q1: X and Y be two random variables.}
  \begin{description}
   \item [(a)]$P(X)$
\begin{table}
\label{q1-table1}
\centering
\caption{P(X) and P(Y)}
\begin{tabular}{|r|r|r|r|r|r|}  \hline
    & X=1 & X=2 & X=3 & P(Y) \\ \hline
Y=a & 0.10 & 0.20 &  0.30 & 0.60 \\ \hline 
Y=b & 0.05 & 0.15 &  0.20 &0.40 \\ \hline
P(X=x) & 0.15 & 0.35 & 0.50 & 1 \\ \hline
\end{tabular}
\end{table}
    Shown in Table 1
    \begin{equation}
     P(X=x) = P(X=1) + P(X=2) + P(X=3) = 0.15 + 0.35 + 0.5 = 1
    \end{equation}
   \item [(b)]$P(Y)$
    Shown in Table 1
    \begin{equation}
      P(Y=x) = P(Y=a) + P(Y=b) = 0.6 + 0.4 = 1
    \end{equation}
   \item [(c)]$P(X \mid Y)$
\begin{table}
\label{q1-table2}
\centering
\caption{$P(X \mid Y)$}
\begin{tabular}{|r|r|r|r|r|}  \hline
    & X=1 & X=2 & X=3 \\ \hline
Y=a & 1/6 & 1/3 &  1/2 \\ \hline 
Y=b & 1/8 & 3/8 &  1/2 \\ \hline
\end{tabular}
\end{table}
     Shown in Table 2
    \begin{equation}
      P(X \mid Y) = P(X | Y=a) + P(X | Y=b)
    \end{equation}
   \item [(d)]$P(Y \mid X)$
\begin{table}
\label{q1-table3}
\centering
\caption{$P(Y \mid X)$}
\begin{tabular}{|r|r|r|r|r|}  \hline
    & X=1 & X=2 & X=3 \\ \hline
Y=a & 2/3 & 4/7 &  3/5 \\ \hline 
Y=b & 1/3 & 3/7 &  2/5 \\ \hline
    \end{tabular}
    \end{table}
    Shown in table 3
   \item [(e)]Are X and Y independent? No since for two random variables to be independent P(X|Y) = P(X) and P(Y|X) = P(Y) and that is not the case. 
   \item [(f)]$H(X)$
    \begin{equation}
      H(X) = -\displaystyle\sum_{x} p(x)\log p(x).
    \end{equation}
    \begin{equation}
     H(X) = H(X=1)+H(X=2)+H(X=3) = 1.44065
   \end{equation}
   \item [(g)]$H(Y)$
   \begin{equation}
     H(Y) = H(Y=a)+H(Y=b) = 0.97095
    \end{equation}
   \item [(h)]$H(X,Y)$
   \begin{equation}
     H(X,Y) = H(X=1,Y=a)+H(X=2,Y=a)+H(X=3,Y=a)
   \end{equation}
\begin{equation}
+H(X=1,Y=b)+H(X=2,Y=b)+H(X=3,Y=b) = 2.408694
\end{equation}
   \item [(i)]$H(X \mid Y)$
	   \begin{equation}
     H(X|Y) = H(X,Y) - H(Y) = 2.408694-0.97095 = 1.4377440000000001
   \end{equation}
   \item [(j)]$H(Y \mid X)$
	   \begin{equation}
     H(Y|X) =  H(X,Y) - H(X) = 2.408694 - 1.44065= 0.9680440000000001
   \end{equation}
   \item [(k)]$MI(X,Y)$
   \begin{equation}
     MI(Y|X) = \displaystyle\sum_{x}\sum_{y}p(x,y)log((p(x,y)/(p(x)p(y)) = 0.002901074507172899
   \end{equation}
   \item [(l1)]What is the value for $KL(P(X,Y) \mid\mid Q(X,Y))$?
      \begin{equation}
     KL(p,q) = \displaystyle\sum_{x}p(x)log2(p(x)/p(q))= 0.10212999408564584
   \end{equation}
   \item [(l2)]What is the value for $KL(Q(X,Y) \mid\mid P(X,Y))$? 
      \begin{equation}
     KL(p,q) = \displaystyle\sum_{x}p(x)log2(p(x)/p(q))= 0.07646881528770542
   \end{equation}
   \item [(l3)]Are they the same? No
  \end{description}
\section{ Q2: Random Variable from coin toss}
 \begin{description}
  \item [(a)]Formula for $H(X)$.
   \begin{equation}
H(X) = -\displaystyle\sum_{x} p(x)\log p(x).
\end{equation}
  \item [(b)]What is $p^*$? \\ .368 with a entropy of 0.530737816926673
  \item [(c)]Prove that the answer you give in (b) is correct. \\ To find the maximum value we can take the derivative of the equation and solve for y= 0. This gives us x = .368. 
   \begin{equation}
H'(X)=\log p(x) +1)/log(2) = 0 
\end{equation}
 \end{description}
\section{ Q3: Permutations and combinations}
 \begin{description}
  \item [(a)]How many distinct ways are there to form the teams for the class(Including formula)? \\For 2, ways = 1, for 4 = 3, 
\begin{equation}
Teams(n) = ((n-1)!/(2!*((n-1)-2)!)
\end{equation}
  \item [(b)]How many different color sequences are there? \\Since each color sequence acts independently the amount of cololor sequences is n! the amount of balls in that color
\begin{equation}
Sequences() = 5!*3!*2! = 1440
\end{equation}
  \item [(c1)]How many different word sequences are there which contain exactly $t_i$ $w_i$'s for each $w_i$ in $\Sigma$? \\ To figure this out I started out with a Vocabulary of 0 and 1s of various sizes and tried to understand the document size and the possibilies and how many documents match the desired conditions. Looking at a bunch of examples I was able to confirm that there are N! different word sequences that contain the correct frequency
  \item [(c2)]What is the probability that you will end up with a document where the occurrence of the word $w_i$ (for each $w_i \in \Sigma$) in the document is exactly $t_i$? \\ 
Where V is the length of the word array
\begin{equation}
P =\prod_{i=1}^n N!/V!
\end{equation}
 \end{description}
\section{ Q4: POS Tagger}
 \begin{description}
  \item [(a)]Write down the formula for calculating $P(w_1, ..., w_n, t_1, ..., t_n)$, 
\begin{equation}
 P =\prod_{i=1}^n P(wi|ti)P(ti|ti-2, ti-1...ti-n) 
\end{equation}
  \item [(b1)]What does each state in HMM correspond to? \\ In an HMM each state corresponds to a word in a sentence or chain. 
  \item [(b2)]How many states are there? \\Depending on how large of a model we want to train whatever our look back window is. N states  
\item [(b3)]What probabilities in the formula for (a) the transition probability is the $p_i$ given $t_(i-1)$  and transmison probability is $t_i$ given $w_i$
 \end{description}
\section{ Q5: POS Classifier}
 \begin{description}
  \item [(a)]How many unique features are there? \\ 4V + $3V^2$
  \item [(b)]What is x? what is y? The X is a current word while the Y is the predicted POS tag for the current word.
  \item [(c)]For the sentence {\bf Mike/NN likes/VBP cats/NNS},write down the feature vector for each word in the sentence.\\ Mike NN PreviousWord BOS CurrentWord Mike NextWord likes surroundingWords (BOS,likes) PreviousTag BOS Prev2tags (BOS,BOS) \\ likes VBP PreviousWord Mike CurrentWord likes NextWord cats surroundingwords (Mike,cats) PreviousTag NN Prev2tags(NN,BOS) \\ cats NNS PreviousWord likes CurrentWord cats surroundingwords (likes,EOS)  PreviousTag VBP Prev2tags(NN, VBP)
 \end{description} 
\section{Q6: Language Identifier}
 \begin{description}
  \item [(a)]How do you plan to build the LangID system? \\ I would treat this as a classification problem. My input would be a document and the output would be a ID which corresponds to a specific langauge. A good set of features would be characters(as in what unicode characters), word unigrams, word bigrams, word trigrams along with POS unigrams, POS bigrams and POS trigrams. 
  \item [(b)]What factors could affect the system performance? \\ The origin of the training data could really affect the training data because some documents in some langauges may come from new and others may come from literature. The system could also be affected by how similair much of the training data is, if its too similair or has too much overlap making a confident model will be trickly. Finally, since my model would use POS tagging to predict langauge the quality of our POS tagger would be hugely impactful. If the POS tagger is low quality tahn we would just learn noise.
 \end{description}
\end{document}



