\documentclass[11pt]{article}
\usepackage{pgfplots}
\usepackage{url}
\pgfplotsset{compat=newest} 
\setlength\topmargin{-0.6cm}   
\setlength\textheight{23.4cm}
\setlength\textwidth{17.0cm}
\setlength\oddsidemargin{0cm} 
\begin{document}
\title{Ling 575 HW8: Language Model enhanced Word Vectors}
\author{Daniel Campos  \tt {dacampos@uw.edu}}
\date{06/03/2019}
\maketitle 
\begin{abstract}
This paper studies the use of emission probabilites between words using OpenAI's GPT2 and BERT implement a better form of Training for Word Vectors. Word2Vec's skipgram model has proven to be quite efficient at representing word relation at a corpus scale but without understanding if the skipgram reltationship is common or quite rare it is difficult to blend the impact of relevent common cooccurences with uncommon rare cooccourences. 
\section{Introduction}
The following the expansion explosion of Word2Vec and Glove the NLP community has rapidly begun to addopt various forms of word embeddings for any kind of task. These embeddings have provided a scalable and efficient way to represent text at scale for any kind of application. More recently, large scale Language Models and sentence represenation have shown even more impresive text represenation but these larger more complicated models are difficult to deploy in resource constraint computting applications. 
\end{document}